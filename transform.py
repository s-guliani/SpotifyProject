# -*- coding: utf-8 -*-
"""Spotify4400.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QkjzYH9KTDEMmJr5iWPnmJX6jYAmo74f

**cleaning the data**
"""

import pandas as pd

# Load the dataset
df = pd.read_csv('dataset.csv')

# Display information about missing values
missing_values = df.isnull().sum()
print("Missing Values:")
print(missing_values)

# Drop rows with missing values (you may choose another strategy)
df = df.dropna()

# Convert data types if needed
df['track_id'] = df['track_id'].astype(str)

# Display information about data types
data_types = df.dtypes
print("\nData Types:")
print(data_types)

missing_values = df.isnull().sum()
print("Missing Values:")
print(missing_values)

# Display information about duplicates
duplicates = df.duplicated()
dup=df[duplicates]
print("\nDuplicates:")
print(duplicates)

print(dup)

# Display descriptive statistics
descriptive_stats = df.describe()
print("\nDescriptive Statistics:")
print(descriptive_stats)

# Save cleaned data to a new file
'''df.to_csv('cleaned_dataset.csv', index=False)
print("\nCleaned data saved to 'cleaned_dataset.csv'")'''

df.dtypes

df.head()

"""***upload to aws bucket***"""

!pip install boto3
!pip install s3fs
!pip install --upgrade boto3
!pip install --upgrade botocore

import pandas as pd
import boto3
import s3fs
from botocore.exceptions import NoCredentialsError

# Function to upload a file to S3#
def upload_to_s3(local_file, bucket_name, s3_file, aws_access_key_id, aws_secret_access_key):
    s3 = boto3.client('s3', aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key)

    try:
        s3.upload_file(local_file, bucket_name, s3_file)
        print(f"Upload successful: {local_file} to {bucket_name}/{s3_file}")
        return True
    except FileNotFoundError:
        print(f"The file {local_file} was not found.")
        return False
    except NoCredentialsError:
        print("Credentials not available.")
        return False

# Replace these variables with your own values
local_file_path = "/content/dataset.csv"
s3_bucket_name = "spotify4400"
s3_key = "landing/transformed_data.csv"
access_key = 'key'
secret_key = 'key'

# Upload the cleaned file to S3
upload_to_s3(local_file_path, s3_bucket_name, s3_key, access_key, secret_key)'''

# Create an S3 filesystem object using s3fs
#s3 = s3fs.S3FileSystem(key=aws_access_key, secret=aws_secret_key)

# Create an S3 client
s3 = boto3.client('s3', aws_access_key_id=access_key, aws_secret_access_key=secret_key)

filename="s3://spotify4400/landing/transformed_data.csv"

from io import BytesIO

response = s3.get_object(Bucket=s3_bucket_name, Key=s3_key)
df = pd.read_csv(BytesIO(response['Body'].read()))

print(df.head())

# Data Transformation Steps
# Convert milliseconds to seconds for duration
df['duration_seconds'] = df['duration_ms'] / 1000

df['duration_seconds']

df['duration_minutes'] = df['duration_ms'] / (60*1000)

df['duration_minutes'] = df['duration_minutes'].round(2)

df['duration_seconds'] = df['duration_seconds'].round(2)

df.head()

df = df.drop(columns=['Unnamed: 0'])

df.head()

df.dtypes

df['duration_seconds'] = df['duration_seconds'].astype(int)
df.head()

df['duration_minutes']=df['duration_minutes'].astype(int)

df.head()

dim_albums=df[['album_name']]
dim_albums

dim_albums = dim_albums.drop_duplicates(subset=['album_name'])
dim_albums

dim_albums['album_id']=dim_albums.reset_index().index+1
dim_albums

dim_albums_cp=dim_albums.copy()
dim_albums_cp

df = pd.merge(df,dim_albums_cp, on='album_name', how='left')
df

#reorder the column to bring id to first column
album_id=dim_albums.columns[-1]
val=dim_albums.pop(album_id)
dim_albums.insert(0,album_id,val)
dim_albums

dim_genres=df[['track_genre']]
dim_genres

dim_genres = dim_genres.drop_duplicates(subset=['track_genre'])
dim_genres

dim_genres['genre_id']=dim_genres.reset_index().index+1
dim_genres

dim_genres_cp=dim_genres.copy()
dim_genres_cp

df = pd.merge(df,dim_genres_cp, on='track_genre', how='left')
df

#reorder the column to bring id to first column
genre_id=dim_genres.columns[-1]
val=dim_genres.pop(genre_id)
dim_genres.insert(0,genre_id,val)
dim_genres

dim_artists=df[['artists']]
dim_artists

dim_artists = dim_artists.drop_duplicates(subset=['artists'])
dim_artists



dim_artists['artists_id']=dim_artists.reset_index().index+1
dim_artists

dim_artists_cp=dim_artists.copy()
dim_artists_cp

df = pd.merge(df,dim_artists_cp, on='artists', how='left')
df

#reorder the column to bring id to first column
artists=dim_artists.columns[-1]
val=dim_artists.pop(artists)
dim_artists.insert(0,artists,val)
dim_artists

dim_artists['artists']= dim_artists['artists'].astype(str)
dim_artists['artists']

facts=['track_id','artists_id','album_id','genre_id','popularity','duration_ms','duration_seconds','duration_minutes','explicit','key','danceability','energy','loudness','mode','speechiness', 'acousticness','instrumentalness','liveness','valence','tempo','time_signature']

dim_facts=df[facts]
dim_facts

dim_facts = dim_facts.drop_duplicates(subset=['track_id'])
dim_facts

dim_facts.info()

dim_facts['explicit']

df['artists'] = df['artists'].str.replace(';', ' ')
df['artists']

from io import StringIO

file_path='processed/dim_artists.csv'

csv_buffer=StringIO()

dim_artists.to_csv(csv_buffer,index=False)

s3.put_object(Bucket=s3_bucket_name, Key=file_path, Body=csv_buffer.getvalue())

response = s3.get_object(Bucket=s3_bucket_name, Key=file_path)
data = response['Body'].read().decode('utf-8')

# %%
status_df=pd.read_csv(StringIO(data))
status_df.info()

"""***Loading Process***"""